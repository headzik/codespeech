\chapter{Related work}
\label{cha:RelatedWork}

This chapter focuses on a short summary of similar projects that have been done in the past and comparing them with the one that is the main topic of this paper. After reading this chapter it should be clear how this project is different from others, what is innovative and what is done in a similar matter. At the end, other important studies related to the topic of programming by voice are discussed. 

\section{VoiceGrip \& VoiceCode}

Désilets et al. describe VoiceCode as ``Innovative Speech Interface for Programming-by-Voice'' \cite{Desilets2006}. After reading their paper and watching an online video  presenting its functionality \cite{VoiceCode2007} it does seem that this is the most complete software in comparison with others. It is not a surprise, seeing how it has been developed through many years, evolving from earlier VoiceGrip project described by Désilets \cite{Desilets2001}. Désilets et al. see the problem in the use of existing SR tools for the purpose of writing code and put focus on providing a way to use more natural utterances. The earlier document has more content and goes through the tasks with which programmers have to deal with in their everyday job and what problems occur in performing them while using SR. VoiceGrip already handles well many of those tasks. The main difference between the two, is that in VoiceGrip, so called pseudo code is being written down word by word and is translated to native code by the Code Translator only when programmer runs it. In VoiceCode the translation is done in real time. Another big modification introduced in the new version was an addition of error correction. It provides two types of error correction: \textbf{Not What I Said}  - when VoiceCode did not recognize the utterance properly and \textbf{Not What I Meant} - to address situations where recognition was correct but translated to the wrong code. This two fold system does not only allow for an easy modification of a mistake but also lets the system to adapt and avoid same mistakes in the future. Both projects allow code navigation. Because it is not clearly stated in what form VoiceCode comes, it is assumed that it is a similar tool as its predecessor, meaning that it can be utilized with different SR systems, such as Dragon NaturallySpeaking, and it can be used by different programming editors, like \eg Emacs. It requires, however, an application of few macros before use. CodeSpeech on the other hand was built as a plugin to commonly used, free of charge IDE - Eclipse and uses built-in SR. It is therefore neither cross SR, nor cross editor. This can come as an advantage or disadvantage with the relation to the other two. First, it is limited only to one IDE and thus cannot be used with another one, possibly preferred by a potential programmer. On the other hand, no special configuration is needed and once the program with plugin is installed, it enables programming by voice with a single click. Another advantage over the aforementioned tools, coming from the choice of a specific, broadly developed IDE is that it is easy to enhance CodeSpeech to execute every single command that exists in Eclipse environment, such as adding breakpoints, compilation, debugging \etc, as well as calling functionality from other plugins, if those are available in the current instance. The software that is the topic of this thesis is just a prototype, therefore it lacks many of the important features that the programs developed by Désilets and the team have, such as sophisticated error correction or transferability across programming languages (VoiceCode can enter specific syntax depending on file extension while CodeSpeech is strictly developed for Java). Many of the provided functionality like local navigation, is still implemented in a limited manner. By the time of writing this thesis neither VoiceGrip nor VoiceCode were available online to try them out.

\section{VocalProgrammer}
Arnold et al. \cite{Arnold2000} have designed a generator of voice recognition syntax-directed programming environments called VocalGenerator. It takes as input context-free grammar for a programming language and a voice vocabulary for that language. Because of that it supports different languages such as Basic, C, C++, Java etc. The authors argue that because of its syntax-directed editing two major capabilities are provided that the standard text editors do not include: navigation and selection. Having that, the authors decided to create another project based on VocalGenerator called VocalProgrammer. This project was supposed to be using using Dragon NaturallySpeaking and Microsoft Visual C++ with focus on Java programming language because its full documentation is available online for free. This tool would require a user to have purchased Dragon NaturallySpeaking Professional Version. Because of no further mention of this project it is assumed that it has never came to a conclusion. 

VocalGenerator is a completely different tool than CodeSpeech. Deriving from description, however, it seems like a similar tool could be developed by it. VocalProgrammer was supposed to be such a tool. It was to be based on Java similarly to CodeSpeech. CodeSpeech does not require any external SR engine as VocalProgrammer would. VocalProgrammer lacks modularity that Eclipse provides, therefore scalability of its functionality seems limited. VocalGenerator allows creation of many editors for different languages, which is a big advantage.

\section{VocalIDE}
VocalIDE is a prototype created in JavaScript created by Rosenblatt et al. \cite{Rosenblatt2018}. It is a web application that allows writing source code using a set of vocal commands. It records and recognizes users' speech using WebkitSpeechRecognition currently available in browsers such as \eg Google Chrome, and rule-based syntax parser. It allows text entry, navigation, text selection, replacement, deletion and snippet entry. Text entry works like in a regular speech recognition program meaning each symbol character has to be explicitly given (``type open parenthesis i space less than space one close parenthesis to enter (i < 1)''). It makes it rather inconvenient to program by voice and is something that CodeSpeech successfully avoided. It is an interesting idea to create such a tool as a web application, which has an advantage of the fact that it can be accessed from everywhere where any kind of computer with a browser is available (which nowadays is rather taken for granted). Requirement for the Internet can be a disadvantage at the same time. CodeSpeech is strictly dependent on a specific program (Eclipse), which has to be installed on a computer first. As for Internet access, for now it is a requirement since it is preferred to use Google Cloud Service, which provides better recognition. Both programs are based on rule-based syntax parser.

Local navigation in both projects is done in a similar manner - the user can say ``go to line'' with a number to jump to a specific line. VocalIDE has developed text selection by word as well as replacement which is something that CodeSpeech lacks so far. Undo operation and smart snippet entry (entering ready code block) are both available in CodeSpeech. Interesting idea presented in this paper is Context Color Editing. Instead of moving the cursor to left or right by one character, nearby words are highlighted in different colors. Saying that color's name moves the active selection to the respective word.

VocalIDE was developed based on the results of research that used Wizard of Oz method. The system was evaluated during the study performed on eight impaired participants. Both researches, as well as the design of a prototype were mostly focused on navigation and selection in Java code.


\section{SPEED: Speech Editor}
SPEED is a program editor described by Begel \cite{Begel2005} which together with program analysis framework called Harmonia \cite{Boshernitsan2001} creates a system embedded in Eclipse IDE. It allows for code creation and edition together with high-level program manipulations like refactoring. To translate voice commands to code it uses SpokenJava - a semantically isomorphic dialect of Java that is more natural to be verbalized \cite{Begel2004}. It allows to replace phrases like ``open braces'' with ``begin class'' in order to create an open bracket after class declaration. That seems to be a big difference between CodeSpeech and SPED, namely, CodeSpeech uses context-free grammar, which once recognized, creates a specific programming construct without the need of handling parenthesis, brackets or explicit starting or ending of these constructs. In order to help people with voice input Begel's project mentioned adding a specific spoken feedback system that would make its use easier to speak proper phrases. Because of strict dependency on grammar, CodeSpeech requires a similar feedback system to help users in learning of the commands for smooth usage. Additionally, a special search and selection tool was being developed for SPED, to help with finding a specific words based on phonetics without spelling a word precisely - similar system is needed for CodeSpeech.

The SPEED system has quite sophisticated feature to handle and support ambiguities through the syntactic phases of program analysis, as well as support for combination of commands and code. It is not clear at which stage of development the project has ended because no further studies on it were found. Just like the rest of the above mentioned projects, this one was not available at the time of writing the thesis. 

\section{NaturalJava}
Shinde describes an interface for programming called NaturalJava \cite{Shinde2017}. It allows entering English sentences as input which produce Java source code as output. It uses information extraction techniques to generate so called case frames that classify key concepts of the sentence. The case frames are then used to modify Abstract Syntax Tree (AST) accordingly. This is the first project (from above mentioned) that uses manipulation of AST rather than being simply text based. Same method is used in CodeSpeech, where specific commands result in creation, deletion or modification of AST nodes. NaturalJava has a very interesting way of interpreting English sentences, which seems to get triggered based on detected keywords such as ``iterates'' in sentence ``create a for loop that iterates from 1 to 5''. CodeSpeech analyses provided utterances via parsed tree walking, and similarly triggers specific actions based on detected keywords. The biggest difference is that NaturalJava does not utilize any speech recognition tool, utterances are to be typed in order to generate code.

\section{CodeTalker}
Snell and Cunningham propose CodeTalker project\cite{Snell2000}. The program was developed as a standalone editor that allowed simple source code creation (in Java) such as definition of a class, function, variable, constructs like for, while loops and switch statements. The editor was created very carefully, iteratively with each version being evaluated in terms of Human Computer Interactions (HCI). The paper also mentions the possibility of creating HTML code.
It contains many useful features, such as replacement of previously defined spoken keywords with text (\eg saying class will insert \texttt{class \{ 
\textbackslash n \}}) and addition of completely new, personal replacements via dialog wizard. It also has an option to highlight syntax, as well as to add specified keywords to be highlighted. Example of spoken input is ``public class my class, braces, private void my function, open brackets, close brackets, braces, count equals my variable, semi-colon''. As can be seen, the punctuation has to be explicitly stated which is something that CodeSpeech wanted to avoid by all means. This, however, allows the CodeTalker to detect if a construct is a class, a variable or a function. In many situations the end of a spoken utterance has to be marked by saying either ``tilde'' to invoke command or `circumflex'' to cancel it. CodeTalker provides a way for entering a word that is reserved as a keyword, which is a common problem in command based systems. Main differences are dependency on other tools and functionality. While CodeSpeech is a plugin that obviously requires Eclipse IDE, CodeTalker is a standalone tool. It requires, however, an external speech recognition software. The former makes operations on AST rather than text, and allows more operations than just creating constructs with specific names but also provides a way to utilize built into Eclipse commands that are important part of the programming, such as compilation and debugging.

\section{Vocal}
An interesting idea of bringing programming by voice to smartphones was brought by Islam et al. \cite{Islam2018}. ``Vocal - a voice based code generator'' is an Android application that enables creation of code by voice. It uses Google Cloud Speech API (similarly to CodeSpeech) as it turned out to be the most accurate from total of free that they have tested out. Only creation of simple structures such as integer or character variables, arrays, method definitions, while and for loops and printing to console is possible in this program. The generated source code is of Java programming language. This software requires an Internet connection to exchange data with the cloud for the feature of compilation on the server. Database is used for storing text files. Although mobiles are not yet suitable for programming, the idea itself is curious. The use of smart devices could potentially make it possible to program conveniently in any place and taking into account Internet accessibility in the world it sounds feasible. Obvious difference between this project and the CodeSpeech is targeted device.

\section{Voice and Gesture Development Environment}

Voice and Gesture Development Environment (VGDE) is an interesting project presented by Bouse \cite{Bouse2017}. It combines together an idea of voice and hand gesture control of the system. Voice commands are used to create and edit code, navigate through it and interact with the program. Gesture commands on the other hand are used only for editing and navigation in the file. This provides a possibility of mixing two methods together as they run in concurrent threads. The author recommends that to achieve the best results one to two second breaks should be done between the commands. This probably makes programming with VDGE quite slow, while in given example ``Bar Equals One Okay'' there are four commands, therefore four breaks are needed. This commands would produce code ``bar = 1;''. The evaluation of VDGE, however, did not measure the system's execution time. To be fair, CodeSpeech also requires around two seconds break between commands as this is the average time of recognition. The difference is, that in the example mentioned above VDGE's four commands correspond to one command in CodeSpeech. \\
Voice recognition is performed by Microsoft Speech and gesture recognition requires external device, namely Leap Motion. Leap Motion is a small device that can be placed between the user and a keyboard. It tracks hands with discrimination of fingers. VDGE unlike CodeSpeech is a standalone development software. Its code manipulation is done on text and does not perform AST operations as CodeSpeech does. It does, however, allow for more detailed navigation in the code. VDGE's targeted programming language is Java, as well as it is of CodeSpeech. Commands are based on grammar and parsed in a similar manner in both programs. Both tools are limited as to the complexity of program that can be created using them, although they do possess the potential for extension.

\section{Voicecode.io}
At the time of writing the thesis one of the most recent programming by voice tools being created was Voicecode.io. The author of the tool is Ben Meyers. The tool on the official website is described as 
\begin{quote}
    \begin{english}
    a concise spoken language that controls your computer in real-time. When writing anything from emails to kernel code, to switching applications or navigating Photoshop - VoiceCode does the job faster and easier. 
    
    VoiceCode is different from other voice-command solutions in that commands can be chained and nested in any combination, allowing complex actions to be performed by a single spoken phrase \cite{Voicecode.io2017}.
    \end{english}
 \end{quote}
From the description it can be concluded that the tool is not a programming tool per se but a general software for system control and it is de facto that. To allow system control Voicecoe.io integrates Dragon NatruallySpeaking SR technology and SmartNav for mouse-free control of the cursor. SmartNav uses a small device that sends infrared waves which reflect from a small pin that can be attached to the baseball cap. This allows for tracking position of the head and moves the cursor accordingly. It is a similar solution to eye tracking technology. The general idea for hand-less system control is great by itself, however what is at the point of interest for the sake of this thesis is its ability to write code. On one of the videos available on official website there is a presentation of an example. The code being written is in JavaScript. Written code is only three lines long and is being written fairly fast. Unfortunately, commands that are spoken to achieve this goal sound nothing like natural English sentences. It is hard to follow and understand which command does what and probably a lot of learning has to be done in order efficient. There is a second video showing writing a script in Ruby, meaning that Voicecode.io is not limited to one programming language, perhaps because it simply writes dictated text (so also punctuations). Currently Voicecode.io supports only MacOS and is constantly under development.

\section{SOPE and ELDI}
An interesting paper written by Bettini and Chin \cite{Bettini1990} comes from year 1990. They describe inside the first results from a research project on man-machine interaction based on speech. SOPE (Speech Oriented Programming Environment) was a project of which the goal was to create programming interface steered by voice. At the beginning the main focus was put on debugging process. In order to do that, IBM's Tangora speech recognition was used and created by them ELDI (English Language Debugger Interface) prototype based on Augmented Transition Networks (ATN). Their goal was to use natural English sentences based on grammar, which at the beginning was quite strict. The restricted vocabulary was up to 60-70 entries. The basic concept they presented is very similar to current solutions, yet at that time as the authors conclude, speech recognition technology was not good enough for this kind of tool to be productive.

\section{Problem of speaking code}

Begel \cite{Begel2005}, Arnold et al. \cite{Arnold2000} , Désilets \cite{Desilets2001} and authors of other previously mentioned papers described problems related to the programming by voice. One of the biggest issues is the fact, that source code is known only in written form and there is no unified way to read it. Every programmer, when asked to read code out loud, has a specific way doing so. Analogically the same problem occurs when the program is supposed to be spoken and then translated into source code as it happens to be with programming by voice. Hermans et al. \cite{Hermans2018} examined vocalization of code on group of novices: ten Dutch high school students that received twenty hours of Python lessons. In the end they did confirm that an issue exists, proposing that a standardize code phonology should be developed in the future. This could not only, as it is stated in the document, help to teach and learn programming, in pairwise programming, but speech recognition based development would benefit as well because each system of that kind could perform the same operations using the same spoken phrases. 

Rodriguez-Cartagena et al. \cite{Rodriguez-Cartagena2015} also make this realization and propose a vocabulary and grammar to help in programming by voice in any C-based language. In order to evaluate, it they performed a study on programming community to receive feedback. 
The programming community consisted of random senior students and faculty members of the Computer Science and Computer Engineering Departments at the Polytechnic University of Puerto Rico and the University of Puerto Rico Río Piedras. This was supposed to be an initial phase for Kavita project, however at the time of writing this thesis the website of the project was unavailable.

\section{Summary}
CodeSpeech has certain commonalities with some of the previously mentioned projects, other things it does different, and certain good ideas could still be used in the future. It was decided for CodeSpeech to mainly perform AST operations rather than simply working on strings of text. This should give better control over the program structure \eg when a whole block is to be copied or deleted, or a child node to be replaced without the need of text selection which in the case of voice control might be clumsy. From above mentioned projects only NaturalJava used the same approach. In the current state CodeSpeech consists of a built-in speech recognition system, while many other works such as VoiceCode or VocalProgrammer use external tools like \eg Dragon NaturallySpeaking. In most cases these tools require an additional purchase from users. The initial idea of CodeSpeech was to be free of charge and therefore integrating an open-source SR tool seemed more viable. Additionally, there exists a possibility to adjust such a tool for higher performance in specialized tasks. The program might be, however, extended at some point to allow external software. CodeSpeech similarly to SPEED is integrated into Eclipse IDE and that makes it more than just a program to create code, but provides a way to expand and use all the functionality of a complete programming environment. Similarly to other works CodeSpeech uses grammar for commands. This might be a bit limiting solution, although it is the easiest to implement. An alternative would be to use a special language analysis tool like it was done in NaturalJava or SPEED. Maybe in the future such a software will replace grammar-based approach, but only if it turns out that it allows for control with more natural sentences. CodeSpeech could greatly benefit from better error correction with automatic ability to improve the results in a manner of VoiceCode. A very interesting idea to integrate usage of gestures was given in VGDE project and perhaps one day it will become a focus of CodeSpeech extension, possibly together with the idea of a different representation of the code (rather than standard text form). Many other aforementioned features have not been integrated due to time limitations but are considered for the future.
