\chapter{Conclusions}
\label{cha:Conclusions}

This chapter starts with a brief reminder of what was the reason behind launching the project in the first place. Later on, it summarizes what has been done until now, describes shortly the different phases of the project development, what is the current status, how well does it satisfy previously specified requirements and if it indeed fulfills its purpose and solves the problems it was created to solve. In the end there is a discussion about potential future work derived from the observations made during the whole process.

\section{Why programming by voice}

There are two main reasons behind the idea of programming by voice. First and foremost are repetitively reported musculoskeletal pains developed by long-term computer users, the author of the thesis being one of them. Some of the issues are suspected to be caused by repetitive movements and are therefore called ``Repetitive Strain Injuries''. There are studies confirming that the pain in upper extremities can be caused by excessive use of mouse and keyboard. If that is so, in order to solve this problem the time of usage of these devices should be limited. 
Another case where programming by voice could become useful deals with a special social group, namely people whose physical impairments do not allow them to use the mouse and keyboard but who are capable of speech. A tool enabling them to use their voices to write programs could help them to take upon an occupation of a programmer. This would supply the ever growing needs of this profession with more experts and at the same time help those people in their lives.

\section{Summary of the development process}
Once the idea came up a way of realization it to life had to be found. After doing some research and reading about similar projects, an initial concept of the project was made. It was decided that the program will be a part of an already existing programming environment, that it will integrate an already existing speech recognition tool, will be developed for Java programming language and be controlled by natural English sentences (also those spoken by a non-native speakers). 


\subsection{Tools selection}
Eclipse IDE was selected as a programming and target platform almost from the very beginning. Main reason being the fact that the author is familiar with this environment, as well as that it can be easily extendable via plugins and it provides a whole toolkit for the development thereof with an extensive documentation. 

The next step was to select speech recognition system, preferably open-source and free of charge. After small research and without getting into details CMUSphinx was decided upon. The system sounded promising and therefore its Java implementation - Sphinx4, was tested out. It turned out not to work so well, and so another of the CMUSphinx tools - Pocketsphinx was given a try, mainly for its keyphrase based recognition. Unfortunately, Pocketsphinx was not easy to apply into Java project and a lot of time was spent for the sole purpose of building the libraries just to be able to use it, and in the end to find out that even in keyphrase mode the speech recognition for non-native speaker is not of satisfying level. In order to improve the results, at first an acoustic model adaptation was performed, then also a new language model was build based on a phonetic dictionary, all in accordance to the instructions contained in the official documentation. Unfortunately, nothing brought expected results and therefore a search for another speech recognition tool was started. As another SR tool to be used, Google Cloud Speech-to-Text service was given a try and it turned out to be a good choice. The results of its recognition allowed for the project to resume its development. It was easy in integration and usage. \\
The time spent on CMUSphinx toolkit could have been spent on implementing more functionality into CodeSpeech project, nonetheless, it is not being considered wasted. The author has learned a lot during that process. A major take away from this experience is to try out different systems before deciding on one, and base the decision on not only the performance, but also on complexity and time needed for integration. 

After the SR API was ready, a decision had to be made on how the voice control of the system should work. The initial idea was to use natural English sentences over command based approach. The reason is obvious, prior to operating with special commands, a learning process is required while in the ideal case a flexible solution could possibly allow to skip this process and let the user trust their intuition to control the program. Unfortunately, there is no unified way of reading code and every person does it in their own manner. Because of that, in order to be able to create a system with natural and intuitive control, a user study in this area has to be done first. To speed things up, the second solution based on command control was settled on. That led to an idea of utilizing grammar and together with it a parser program to interpret it. Commands defined by grammar turned out to allow for some flexibility, even more so with ANTLR being a parsing tool, as it allows for different implementations of so called (keyword) listeners that act differently on detected tokens throughout the process of traversing through a phrase. With more work and more options added, a system close to an ideal one might be possible to achieve. For now the expected input is rather strict, but still able to perform its task well enough for the first iteration.

\subsection{Architecture design}
Once all of the tools were ready, the time came to integrate them together to create a new product. The basic concept was already there, yet it was lacking necessary details. How will the different tools to communicate, what will be their relation \etc were the questions that had to be answered. Each tool was planned to be implemented separately with different responsibilities. Some of the answers came straight from the tools themselves. For example, the program was supposed to be an Eclipse plugin. That laid the foundations for the architecture. Basically what was needed to do was to follow an official documentation on plug-in development. It was then clear, that the main structure will be a class that is an instance of a plug-in itself. This instance was decided to be a ``headquarters'' for the rest of the tools, over which it had control. \\
Workflow between different parts was more or less predetermined, as at first speech has to be recognized and only then could be parsed and acted upon. However, instead of the result of recognition being directly passed to another tool for interpretation, it was decided to make it go through the plugin instance. This is to decrease coupling and give more control to the main class. Communication between different parts of the project is done via event system. Whenever an action that triggers a specific event happens, all of the registered listeners receive a notification with important data. The main listener of both, recognition and interpretation is the plugin class. 

The hardest part was to come up with a way to go from interpretation to performing a task. Both of these processes are related, because action to be performed depends directly on the interpretation of the command. This relation and complexity led to constant changes during the development process. \\
The main problem was the sequence of words in natural English sentences on which commands are based versus the sequence of performing a task in programming. For easier understanding of the issue an explanation is given on an example. Usually, the process of performing a task is in this order: an object of a specific \textbf{type} and \textbf{name} is created, then all of its \textbf{attributes} are set and only when the object is ready it is used to perform a certain \textbf{task}. Now, given a phrase ``create public static method main'' it can be seen, that the verb determining the \textbf{task} to be performed comes first (create), right after that there are \textbf{attributes} (public, static), then there is a word defining the \textbf{type} to be created (method) and finally there is a \textbf{name} (main). The spoken English sentence is in reversed order.

Finally, understanding that the troublesome order of keywords and the sequential nature of the parser make it difficult to instantaneously perform correct operation, the full advantage of ANTLR's flexibility was taken and a build up approach was introduced. Because each of the words might have a different interpretation depending on the context it was decided to use different listeners. The first verb determines which listener will be used in parsing. Then throughout the whole process an interpreter context object gathers all of the necessary information such as attributes during the whole process of interpretation. Once a structure keyword is recognized it initializes a specific operation based on the type of a task and this structure. Structures are represented by models that consists of all attributes important for them. 

Before settling for the operations and models, other ideas were tampered with. Initially, all operations were supposed to be performed by specific objects called managers. The problem appeared then, how to group them. Should they be focused on operation, like a \texttt{CreationManager} responsible for creation of all of the structures, or should they be structure focused, \eg \texttt{MethodManager} performing all operations on methods. What's with operations not involved with code? To perform operations on AST there would be an \texttt{ASTManager} and to perform operations on UI - \texttt{UIManager} (the last two actually made it to the final version, with some modifications). It was noticed then that the problem can be solved by implementing the ``double dispatch" through the Visitor pattern. Performed operation would depend on the types of two instances: operation and a model. During the process of parsing the command, proper model and operation objects would be created, such as \texttt{CreateOperation} and \texttt{VariableModel}. Then a proper overloaded method of \texttt{perform} of \texttt{CreateOperation} taking \texttt{VariableModel} as parameter would be invoked. After some time the program grew, and with it more and more models were added, resulting in many inevitable changes in operation classes, ending with few of them having empty not implemented methods, and others not being dependent on models at all. That led a will to simplify the structure, keeping the concept of operations and models but resigning from the Visitor pattern. Right now each operation class performs only a single action taking a single object passed as a parameter. The object has to be of proper type but that is being handled inside its method. This results in compact, but many operation classes. Whether this is a good structure or not is debatable, the author himself is not certain of that. Nevertheless, it turned out that management of such classes is easy (unless the abstraction needs to be changed, following by the need of changing every single realization) and the addition of new functionalities is as simple.

\subsection{Implementation}
Design and implementation phases were constantly being changed. At first, a basic architecture was designed, then when it came to the implementation some new ideas came up, and so the design was modified. An example of this process was presented in the previous subsection. A slight different, but worth mentioning experience that the author had was introduction of abstractions. The search for a satisfying SR API resulted in having three implementations of different technologies that were supposed to do the same. Obviously a need for unification appeared. That lead to creation of an abstraction for speech recognition, with each SR implemented so far being a realization of it. Combining this with a Factory design pattern, it not only made it possible to easily switch between different APIs, but also to extend the project by new ones. This experience can be a real life example of the advantage that working with abstractions bring. Of course sometimes it is not easy to design such a system at the very beginning, in the first place the designer has to be familiar with the general idea of how the system should work. This is what has happened in this case. Working with different APIs at the beginning helped the author to gain an understanding of how the technology works and therefore made it easier to derive an abstraction.

This iterative approach was a repeating pattern throughout the whole creation process in a fashion of agile development, with each iteration going back to the previous phases if necessary.

\section{Conclusions}

The final product satisfies most of the requirements at least in partially, but unfortunately not all of them due to time limitations. From the functional requirements, what is lacking is an option to run and debug the program. Nevertheless, extending the project by these two features should be quite easily doable. The rest of the functionality is implemented, at least in a minimal form. Of course the terms used in the requirements are quite broad, because under the phrase ``create source code'' lie many features, such as creation of variables, methods \etc, which could be a separate requirement on its own. For the purpose of simplification those specific requirements were generalized and put into one category.

Non-functional requirements are also not entirely fulfilled. For example, the program is easy to use, however it still requires a bit of learning due to the strict commands. It does recognize English sentences, even those spoken by non-native speakers. The system is rather reliable, meaning that it does not lose data and does not crash. There are times when exceptions are thrown, but they are being caught and thus the usage of the program can be continued. In some cases it is possible to correct mistakes, but this feature still could be improved. If the program is intuitive is unknown and to check that a user study has to be performed. The system does not inform about the errors unfortunately, or gives any feedback for that matter (except when the operation is performed). This was broadly discussed in one of the previous subsections in potential extension by feedback system. The response of the project is relatively fast, but is not immediate and that is due to the recognition process which in average takes around 2 seconds. The system was tested under Windows 10 and because the program is an Eclipse plugin and Eclipse is available for Linux it is assumed that it will work there as well, however, this is yet to be tested.

In the end a prototype of a programming by voice tool was created. At first the functional and non-functional requirements were defined together with planned use cases. Then, an initial structure was designed and in iterative process of agile development the first version of the tool was finished. After that, an evaluation of the system was done by the author. The arising results showed that in the current state the prototype is not ready to completely replace the standard way of programming through the use of mouse and keyboard, but it can decrease the amount of interaction with these devices and thus possibly minimize the risk of RSI occurrence. It is worth to remind that the program under test is still just a prototype and has plenty of room for improvement. It is assumed that once it is enhanced and it provides more features it could become a substitution for mouse and keyboard in the future.


\section{Future work}

From this point on there are a couple of things that need to be done. The first one is to of course enhance the project further following some of the ideas given in the previous chapter. During the evaluation a realization was made, of which features should be implemented first in order to speed up the increase of project's performance. Ideally, the second iteration should already allow creation of a complete program done entirely by voice, without the need of mouse and keyboard.

In parallel to the development of the second iteration, a user study should be performed in order to answer the question of how the voice input should look like according to the programmers. The research could take the form of Wizard of Oz study, in which the users sit in front of a workstation. They do not need to interact with the workstation manually at all. In order to perform an action the users speak out loud what action they want to perform and these actions are then performed by another person at another computer which shares the screen to the test station. All changes are being displayed to the user in real time. This way the subjects under study will have a real feeling of operating by voice. Phrases spoken by them will be recorded and analyzed. This study will help to form a grammar in such a way to allow many different possibilities of performing the same task using different sentences, thus making program operation feels more natural and therefore decreasing the time needed for learning, ideally to zero. 

Once the second iteration is ready and the results of the user study are applied to it, another evaluation of the project should be done. This time it would involve more people, ideally around twenty. All of the participants should be familiar with the concept of Object Oriented Programming and Java programming language, preferably with an Eclipse IDE as well. In this study the users will have to write the same program using standard method and voice (similarly to the way it has been done in a test described in this thesis). In order to nullify learning effect of the program, the participants will be divided into two groups. The first group will begin with coding in a standard manner and the second one, with voice. If needed, a list of possible commands can be presented to study subjects either before the start or can be made available for the whole time (perhaps implemented into the system as an extra feature). Both methods will be then compared as to the time of completion. In addition, CER will be independently measured. After the study a questionnaire will be provided to the participants in order to gather information of subjective feelings about the system, such as if it is easy, intuitive, frustrating and if they would be willing to use it in the future.  

Next user study will determine how the learning factor affects the performance. The idea as of now is to repeat the user study ten times over with the same study group but with different example programs. Each time will be recorded and the results will be compared. This will show if there is a potential for programming by voice to increase the performance to such an extent that it can be competitive to the standard way, or perhaps even better.

Changing the way of interaction with the computer might influence programming in a great manner. In case voice coding becomes the main method of writing programs, it might not be feasible to do so in a form of text. Using specified commands and manipulating Abstract Syntax Tree might be a better solution. As an effect this could also have consequences in the representation of the program. When the text files is no more needed, perhaps some sort of graphical representation would become a natural replacement. This is an interesting topic for Human-Computer Interaction research and once everything else is done and the program is completed, a window will open for scientific studies in this field.